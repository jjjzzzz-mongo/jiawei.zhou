{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch From Trino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script establishes a connection to a Presto database via the Trino Python client, \n",
    "executes a SQL query to fetch data, and then structures the fetched data into a pandas DataFrame.\n",
    "The DataFrame is Clusters with appropriate column names, making the data ready for analysis.\n",
    "\"\"\"\n",
    "\n",
    "import trino\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Connect to Trino\n",
    "conn = trino.dbapi.connect(\n",
    "    host=\"presto-gateway.corp.mongodb.com\",\n",
    "    port=443,\n",
    "    user=\"jiawei.zhou@mongodb.com\",\n",
    "    catalog=\"awsdatacatalog\",\n",
    "    http_scheme=\"https\",\n",
    "    auth=trino.auth.BasicAuthentication(\n",
    "        \"jiawei.zhou@mongodb.com\", \"Youarethebest@1016\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Execute query\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    f\"\"\"\n",
    "select\n",
    "\t*\n",
    "from\n",
    "\tawsdatacatalog.product_analytics.pnl_products_adoption_qualification_activation\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Extract column names\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Fetch rows\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Create DataFrame\n",
    "df_raw = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Convert 'ds' column to pandas datetime\n",
    "df_raw[\"ds\"] = pd.to_datetime(df_raw[\"ds\"])\n",
    "\n",
    "# Now perform the filtering with no type mismatch\n",
    "df_raw = df_raw[\n",
    "    (df_raw[\"ds\"] >= pd.Timestamp(\"2023-02-01\"))\n",
    "    & (df_raw[\"ds\"] <= pd.Timestamp(\"2024-02-01\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the maximum number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "# Set the maximum number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# # Base product name\n",
    "# product = \"text_search\"\n",
    "\n",
    "# Dynamically generate column names based on the product\n",
    "product_ds = f\"{product}_ds\"\n",
    "product_ind = f\"{product}_ind\"\n",
    "\n",
    "# Print the results to verify\n",
    "print(product_ds)\n",
    "print(product_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "data = df_raw.copy()\n",
    "\n",
    "# Display rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ds\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set pandas option to display float format without scientific notation\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "# Use the describe function and print the result\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null & 0 Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the total number of clusters (rows in your DataFrame)\n",
    "total_clusters = data.shape[0]\n",
    "\n",
    "# Calculate the null values count for each column\n",
    "null_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of null values for each column\n",
    "null_values_percentage = (null_values / total_clusters) * 100\n",
    "\n",
    "# Calculate the 0 values count for each column\n",
    "zero_values = (data == 0).sum()\n",
    "\n",
    "# Calculate the percentage of 0 values for each column\n",
    "zero_values_percentage = (zero_values / total_clusters) * 100\n",
    "\n",
    "# Create a DataFrame to display the null values count and percentage\n",
    "null_values_df = pd.DataFrame(\n",
    "    {\"Null Count\": null_values, \"Null Percentage\": null_values_percentage}\n",
    ")\n",
    "\n",
    "# Create a DataFrame to display the 0 values count and percentage\n",
    "zero_values_df = pd.DataFrame(\n",
    "    {\"Zero Count\": zero_values, \"Zero Percentage\": zero_values_percentage}\n",
    ")\n",
    "\n",
    "# Merge the two DataFrames for a comprehensive view\n",
    "values_df = pd.concat([null_values_df, zero_values_df], axis=1)\n",
    "\n",
    "# Format the percentage values to string with a '%' sign for better readability\n",
    "values_df = values_df.applymap(lambda x: f\"{x:.2f}%\" if isinstance(x, float) else x)\n",
    "\n",
    "# Display rows where the Null Count or Zero Count is greater than 0\n",
    "filtered_values_df = values_df[\n",
    "    (values_df[\"Null Count\"] > 0) | (values_df[\"Zero Count\"] > 0)\n",
    "]\n",
    "\n",
    "# If you want to see the result\n",
    "filtered_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the total number of clusters (rows in your DataFrame)\n",
    "total_clusters = data.shape[0]\n",
    "\n",
    "# Calculate the percentage of null values for each column\n",
    "null_values_percentage = (data.isnull().sum() / total_clusters) * 100\n",
    "\n",
    "# Identify columns to exclude from dropping (i.e., keep these columns regardless of their null percentage)\n",
    "columns_to_keep = [product_ds, product_ind]\n",
    "\n",
    "# Find columns with >= 40% null values, excluding the specified columns\n",
    "columns_to_drop = [\n",
    "    col\n",
    "    for col in null_values_percentage.index\n",
    "    if null_values_percentage[col] >= 40 and col not in columns_to_keep\n",
    "]\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'ds' to datetime format\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "\n",
    "# Filtering the data for 'ds = 2023-02-01'\n",
    "filtered_data = data[data[\"ds\"] == \"2023-02-01\"]\n",
    "\n",
    "# Update the lists of features as per your specification\n",
    "categorical_features = [\n",
    "    \"org_plan_type\",\n",
    "    \"is_cross_region\",\n",
    "    \"instance_size\",\n",
    "    \"topology\",\n",
    "    \"cluster_mdb_major_version\",\n",
    "    \"org_country\",\n",
    "    \"email_segment_clean\",\n",
    "    \"marketing_channel_group\",\n",
    "    \"account_segment\",\n",
    "    \"self_serve_or_sales_sold\",\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"cluster_age_month\",\n",
    "    \"disk_size_gb\",\n",
    "    \"org_latest_mrr\",\n",
    "    \"reads_per_second_monthly_daily_avg\",\n",
    "    \"writes_per_second_monthly_daily_avg\",\n",
    "    \"connections_monthly_daily_avg\",\n",
    "    \"opcounter_cmd_monthly_daily_avg\",\n",
    "    \"db_data_size_monthly_daily_avg\",\n",
    "    \"cluster_mrr_total\",\n",
    "]\n",
    "\n",
    "# Calculate the number of plots needed\n",
    "num_plots = len(categorical_features) + len(numerical_features)\n",
    "cols = 2\n",
    "rows = num_plots // cols + (num_plots % cols > 0)\n",
    "\n",
    "plt.figure(\n",
    "    figsize=(16, max(5 * rows, 20))\n",
    ")  # Adjusting the figure size based on the number of plots\n",
    "\n",
    "# Plotting histograms for the new set of numerical variables\n",
    "for i, var in enumerate(numerical_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(filtered_data[var], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {var} for 2023-02-01\")\n",
    "\n",
    "# Plotting bar plots for the new set of categorical variables\n",
    "for j, var in enumerate(categorical_features, len(numerical_features) + 1):\n",
    "    plt.subplot(rows, cols, j)\n",
    "    order = filtered_data[var].value_counts().index\n",
    "    sns.countplot(y=var, data=filtered_data, order=order)\n",
    "    plt.title(f\"Distribution of {var} for 2023-02-01\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the distribution of the product_ind target variable over the 'ds' date\n",
    "target_distribution_over_ds = (\n",
    "    data.groupby(\"ds\")[product_ind].value_counts().unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Convert the index (which is 'ds') to datetime objects first\n",
    "target_distribution_over_ds.index = pd.to_datetime(target_distribution_over_ds.index)\n",
    "\n",
    "# Now, you can safely extract the date part only to ignore the time part\n",
    "target_distribution_over_ds.index = target_distribution_over_ds.index.date\n",
    "\n",
    "# Use the parameter in the plot\n",
    "target_distribution_over_ds.plot(kind=\"bar\", stacked=True, figsize=(10, 5))\n",
    "\n",
    "# Integrate the product_title into the title and legend labels\n",
    "plt.title(f\"Distribution of {product} Indicator Over Date\")\n",
    "plt.xlabel(\"Date (ds)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(\n",
    "    labels=[f\"w/o {product}\", f\"w/ {product}\"],\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Treatment & Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "data = df_raw.copy()\n",
    "\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "data[product_ds] = pd.to_datetime(data[product_ds])\n",
    "\n",
    "# Determine the earliest product_ds date for each cluster\n",
    "earliest_product = data.groupby(\"cluster_id\")[product_ds].min().reset_index()\n",
    "\n",
    "\n",
    "def assign_group(row):\n",
    "    # Convert string dates to datetime objects for comparison\n",
    "    start_date = pd.to_datetime(\"2023-02-01\")\n",
    "    end_date = pd.to_datetime(\"2023-12-31\")\n",
    "\n",
    "    if pd.isnull(row[product_ds]):\n",
    "        return \"control group\"\n",
    "    elif row[product_ds] < start_date or row[product_ds] > end_date:\n",
    "        return \"not eligible\"\n",
    "    elif start_date <= row[product_ds] <= end_date:\n",
    "        return \"treatment group\"\n",
    "    else:\n",
    "        return \"control group\"\n",
    "\n",
    "\n",
    "earliest_product[\"group_assignment\"] = earliest_product.apply(assign_group, axis=1)\n",
    "\n",
    "\n",
    "# Merge the group assignment back to the original dataset\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    earliest_product[[\"cluster_id\", \"group_assignment\"]],\n",
    "    on=\"cluster_id\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure product_ds is in datetime format\n",
    "data[product_ds] = pd.to_datetime(data[product_ds], errors=\"coerce\")\n",
    "\n",
    "# 1. Count of all unique clusters\n",
    "total_unique_clusters = data[\"cluster_id\"].nunique()\n",
    "\n",
    "# 2. Count of unique clusters with product_ind equal to 1\n",
    "unique_clusters_product_ind_1 = data[data[product_ind] == 1][\"cluster_id\"].nunique()\n",
    "\n",
    "# 3. Count of unique clusters with product_ds >= '2023-02-01' and <= '2023-12-31'\n",
    "unique_clusters_product_ds = data[\n",
    "    (data[product_ds] >= \"2023-02-01\") & (data[product_ds] <= \"2023-12-31\")\n",
    "][\"cluster_id\"].nunique()\n",
    "\n",
    "# 4. Count of unique clusters in each 'group_assignment'\n",
    "unique_clusters_per_group_assignment = data.groupby(\"group_assignment\")[\n",
    "    \"cluster_id\"\n",
    "].nunique()\n",
    "\n",
    "# Display the results with dynamic column names\n",
    "print(f\"Total number of unique clusters: {total_unique_clusters}\")\n",
    "print(f\"Unique clusters with '{product_ind}' = 1: {unique_clusters_product_ind_1}\")\n",
    "print(\n",
    "    f\"Unique clusters with '{product_ds}' >= '2023-02-01' and < '2023-12-31': {unique_clusters_product_ds}\"\n",
    ")\n",
    "print(\"Unique clusters in each 'group_assignment':\")\n",
    "for group_assignment, count in unique_clusters_per_group_assignment.items():\n",
    "    print(f\"  - {group_assignment}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'ds' and product_ds to datetime format in one step\n",
    "data[[\"ds\", product_ds]] = data[[\"ds\", product_ds]].apply(pd.to_datetime)\n",
    "\n",
    "# Define the filtering conditions in advance to improve readability\n",
    "eligible = data[\"group_assignment\"] != \"not eligible\"\n",
    "date_range = (data[\"ds\"] >= \"2023-02-01\") & (data[\"ds\"] <= \"2023-12-31\")\n",
    "not_deleted = data[\"is_deleted\"] != True  # Add not deleted filter\n",
    "mrr_greater_than_zero = data[\"cluster_mrr_total\"] > 0  # Add MRR > 0 filter\n",
    "\n",
    "# Apply all filtering conditions\n",
    "data = data[eligible & date_range & not_deleted & mrr_greater_than_zero]\n",
    "\n",
    "# Use a single line to create 'same_month_indicator' by comparing years and months directly\n",
    "data[\"same_month_indicator\"] = (\n",
    "    (data[\"ds\"].dt.to_period(\"M\") == data[product_ds].dt.to_period(\"M\"))\n",
    ").astype(int)\n",
    "\n",
    "# Convert 'ds' to a 'month_year' period format for monthly analysis directly\n",
    "data[\"month_year\"] = data[\"ds\"].dt.to_period(\"M\")\n",
    "\n",
    "# Set 'product_month_indicator' using period comparison for simplicity\n",
    "data[\"product_month_indicator\"] = (\n",
    "    data[product_ds].notnull()\n",
    "    & (data[product_ds].dt.to_period(\"M\") <= data[\"month_year\"])\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'product_ind' and count unique 'cluster_id's\n",
    "unique_clusters_product_ind = data.groupby(product_ind)[\"cluster_id\"].nunique()\n",
    "\n",
    "# Use the dynamically generated 'product_ind' in the print statement\n",
    "print(f\"Unique clusters by {product_ind}:\\n\", unique_clusters_product_ind)\n",
    "\n",
    "# Group by 'same_month_indicator' and count unique 'cluster_id's\n",
    "unique_clusters_same_month_indicator = data.groupby(\"same_month_indicator\")[\n",
    "    \"cluster_id\"\n",
    "].nunique()\n",
    "print(\n",
    "    \"\\nUnique clusters by same_month_indicator:\\n\", unique_clusters_same_month_indicator\n",
    ")\n",
    "\n",
    "# Group by 'group_assignment' and count unique 'cluster_id's\n",
    "unique_clusters_group_assignment = data.groupby(\"group_assignment\")[\n",
    "    \"cluster_id\"\n",
    "].nunique()\n",
    "print(\"\\nUnique clusters by group_assignment:\\n\", unique_clusters_group_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'ds' column to datetime format\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "\n",
    "# Filter data for a specific date\n",
    "filtered_data = data[data[\"ds\"] == \"2023-02-01\"]\n",
    "\n",
    "# Count missing values by 'group_assignment'\n",
    "missing_values = filtered_data.isnull().groupby(filtered_data[\"group_assignment\"]).sum()\n",
    "\n",
    "# Add a row to show total missing values for each column\n",
    "missing_values.loc[\"Total Missing\"] = missing_values.sum()\n",
    "\n",
    "# Calculate and print the total number of unique clusters per 'group_assignment'\n",
    "total_clusters = filtered_data.groupby(\"group_assignment\")[\"cluster_id\"].nunique()\n",
    "\n",
    "# Print the missing values table\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = [\n",
    "    \"cluster_age_month\",\n",
    "    \"org_plan_type\",\n",
    "    \"is_cross_region\",\n",
    "    \"instance_size\",\n",
    "    \"topology\",\n",
    "    \"cluster_mdb_major_version\",\n",
    "    \"disk_size_gb\",\n",
    "    \"org_country\",\n",
    "    \"org_latest_mrr\",\n",
    "    \"email_segment_clean\",\n",
    "    \"marketing_channel_group\",\n",
    "    \"self_serve_or_sales_sold\",\n",
    "    \"reads_per_second_monthly_daily_avg\",\n",
    "]\n",
    "\n",
    "# Ensure 'ds' column is in datetime format (if not already done)\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "\n",
    "# Filter the DataFrame for rows where 'ds' equals '2023-02-01'\n",
    "filtered_data = data[data[\"ds\"] == \"2023-02-01\"]\n",
    "\n",
    "# Calculate the percentage of rows with any missing value, grouped by 'group_assignment'\n",
    "# Only considering the filtered data for '2023-02-01'\n",
    "percentage_missing = (\n",
    "    filtered_data[features]\n",
    "    .isnull()\n",
    "    .any(axis=1)\n",
    "    .groupby(filtered_data[\"group_assignment\"])\n",
    "    .mean()\n",
    "    * 100\n",
    ")\n",
    "\n",
    "# Format the output with a '%' sign\n",
    "formatted_percentage_missing = percentage_missing.apply(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# Display the formatted percentages\n",
    "formatted_percentage_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Regression & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. try without smote\n",
    "# 2. xgboost no need for the standardization\n",
    "# 3. shapley value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Filter dataset for ds='2023-12-01'\n",
    "data_filtered = data[data[\"ds\"] == \"2023-12-01\"]\n",
    "\n",
    "X = data_filtered[features]\n",
    "y = data_filtered[\"product_month_indicator\"]\n",
    "\n",
    "# Drop rows with any null values in the features\n",
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Apply Label Encoding to categorical features\n",
    "le = LabelEncoder()\n",
    "for column in X.select_dtypes(include=[\"object\"]).columns:\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Address imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Logistic Regression classifier within a pipeline that includes scaling\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[\n",
    "    :, 1\n",
    "]  # Probability estimates for the positive class\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate and print additional evaluation metrics\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Cohenâ€™s Kappa: {cohen_kappa:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "# Extract the logistic regression model from the pipeline\n",
    "logistic_regression_model = model.named_steps[\"logisticregression\"]\n",
    "\n",
    "# Get the coefficients from the logistic regression model\n",
    "coefficients = logistic_regression_model.coef_[0]\n",
    "\n",
    "# Create a DataFrame to display feature names and their corresponding coefficients\n",
    "feature_importance = pd.DataFrame({\"Feature\": X.columns, \"Coefficient\": coefficients})\n",
    "\n",
    "# Calculate the absolute values of coefficients to determine their impact regardless of direction (positive/negative)\n",
    "feature_importance[\"Absolute_Coefficient\"] = feature_importance[\"Coefficient\"].abs()\n",
    "\n",
    "# Sort the features by their absolute coefficient values in descending order\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by=\"Absolute_Coefficient\", ascending=False\n",
    ")\n",
    "\n",
    "# Display the sorted feature importance\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact + Fuzzy Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_match_variable = \"month_year\"\n",
    "final_refine_variable = \"cluster_mrr_total\"\n",
    "match_order = feature_importance['Feature'].head(6).tolist()\n",
    "# Directly specifying the match_order list with predefined order\n",
    "# match_order = [\n",
    "#     \"org_latest_mrr\",\n",
    "#     \"instance_size\",\n",
    "#     \"org_plan_type\",\n",
    "#     \"self_serve_or_sales_sold\",\n",
    "#     \"topology\",\n",
    "#     \"cluster_age_month\",\n",
    "# ]\n",
    "\n",
    "data.loc[:, \"pair_index\"] = pd.NA\n",
    "global_pair_index = 0\n",
    "\n",
    "unique_months = data[base_match_variable].drop_duplicates()\n",
    "\n",
    "for month in unique_months:\n",
    "    month_data = data[data[base_match_variable] == month].copy()\n",
    "\n",
    "    # Remove outliers for 'cluster_mrr_total'\n",
    "    Q1 = month_data[final_refine_variable].quantile(0.25)\n",
    "    Q3 = month_data[final_refine_variable].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # Keep rows that are not considered outliers\n",
    "    month_data = month_data[\n",
    "        (month_data[final_refine_variable] >= (Q1 - 1.5 * IQR))\n",
    "        & (month_data[final_refine_variable] <= (Q3 + 1.5 * IQR))\n",
    "    ]\n",
    "\n",
    "    treatment_data = month_data[\n",
    "        (month_data[\"group_assignment\"] == \"treatment group\")\n",
    "        & (month_data[\"same_month_indicator\"] == 1)\n",
    "    ]\n",
    "    control_data = month_data[month_data[\"group_assignment\"] == \"control group\"].copy()\n",
    "\n",
    "    for t_index, t_row in treatment_data.iterrows():\n",
    "        potential_matches = control_data.copy()\n",
    "        match_found = False  # Flag to indicate if a match has been found\n",
    "\n",
    "        for feature in match_order:\n",
    "            initial_potential_matches = (\n",
    "                potential_matches.copy()\n",
    "            )  # Keep a copy before filtering\n",
    "\n",
    "            if feature in [\"org_latest_mrr\", \"cluster_age_month\"]:  # Numerical features\n",
    "                match_value = t_row[feature]\n",
    "                potential_matches = potential_matches[\n",
    "                    (potential_matches[feature] >= match_value * 0.9)\n",
    "                    & (potential_matches[feature] <= match_value * 1.1)\n",
    "                ]\n",
    "            else:  # Categorical features\n",
    "                potential_matches = potential_matches[\n",
    "                    potential_matches[feature] == t_row[feature]\n",
    "                ]\n",
    "\n",
    "            if potential_matches.empty:\n",
    "                potential_matches = initial_potential_matches  # Restore before filtering if no match found\n",
    "            else:\n",
    "                match_found = True  # Update flag when matches are found\n",
    "\n",
    "            if len(potential_matches) < 3:\n",
    "                break\n",
    "\n",
    "        # If matches are found for any feature\n",
    "        if match_found and not potential_matches.empty:\n",
    "            potential_matches[\"difference\"] = abs(\n",
    "                potential_matches[final_refine_variable] - t_row[final_refine_variable]\n",
    "            )\n",
    "            closest_match_index = potential_matches[\"difference\"].idxmin()\n",
    "\n",
    "            # Update 'pair_index' for both treatment and matched control\n",
    "            data.loc[t_index, \"pair_index\"] = global_pair_index\n",
    "            data.loc[closest_match_index, \"pair_index\"] = global_pair_index\n",
    "            global_pair_index += 1  # Increment for the next pair\n",
    "\n",
    "            # Remove the matched control to avoid reusing\n",
    "            control_data = control_data.drop(closest_match_index)\n",
    "\n",
    "# Convert 'pair_index' to Int64 and filter unmatched entries\n",
    "data[\"pair_index\"] = data[\"pair_index\"].astype(\"Int64\")\n",
    "final_matched_dataset = data.dropna(subset=[\"pair_index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique clusters in the treatment group from original data\n",
    "unique_clusters_treatment_original = final_matched_dataset[\n",
    "    final_matched_dataset[\"group_assignment\"] == \"treatment group\"\n",
    "][\"cluster_id\"].nunique()\n",
    "print(f\"Unique clusters in treatment: {unique_clusters_treatment_original}\")\n",
    "\n",
    "# Calculate unique clusters in both groups from final_matched_dataset\n",
    "unique_clusters_treatment_final = final_matched_dataset[\n",
    "    final_matched_dataset[\"group_assignment\"] == \"treatment group\"\n",
    "][\"cluster_id\"].nunique()\n",
    "unique_clusters_control_final = final_matched_dataset[\n",
    "    final_matched_dataset[\"group_assignment\"] == \"control group\"\n",
    "][\"cluster_id\"].nunique()\n",
    "print(\n",
    "    f\"Unique clusters in treatment: {unique_clusters_treatment_final}, control: {unique_clusters_control_final}\"\n",
    ")\n",
    "\n",
    "# Calculate total cluster counts in both groups\n",
    "total_clusters_treatment_final = final_matched_dataset[\n",
    "    final_matched_dataset[\"group_assignment\"] == \"treatment group\"\n",
    "][\"cluster_id\"].count()\n",
    "total_clusters_control_final = final_matched_dataset[\n",
    "    final_matched_dataset[\"group_assignment\"] == \"control group\"\n",
    "][\"cluster_id\"].count()\n",
    "print(\n",
    "    f\"Total clusters in treatment: {total_clusters_treatment_final}, control: {total_clusters_control_final}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = [\n",
    "    \"org_plan_type\",\n",
    "    \"is_cross_region\",\n",
    "    \"instance_size\",\n",
    "    \"topology\",\n",
    "    \"cluster_mdb_major_version\",\n",
    "    \"org_country\",\n",
    "    \"email_segment_clean\",\n",
    "    \"marketing_channel_group\",\n",
    "    \"account_segment\",\n",
    "    \"self_serve_or_sales_sold\",\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"cluster_age_month\",\n",
    "    \"disk_size_gb\",\n",
    "    \"org_latest_mrr\",\n",
    "    \"reads_per_second_monthly_daily_avg\",\n",
    "    \"writes_per_second_monthly_daily_avg\",\n",
    "    \"connections_monthly_daily_avg\",\n",
    "    \"opcounter_cmd_monthly_daily_avg\",\n",
    "    \"db_data_size_monthly_daily_avg\",\n",
    "    \"cluster_mrr_total\",\n",
    "]\n",
    "\n",
    "# Filter data for Feb 2023\n",
    "feb_2023_data = data[\n",
    "    (data[\"ds\"].dt.year == 2023)\n",
    "    & (data[\"ds\"].dt.month == 2)\n",
    "    & (\n",
    "        (data[\"group_assignment\"] == \"treatment group\")\n",
    "        | (data[\"group_assignment\"] == \"control group\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# Plotting categorical features with count plots\n",
    "fig, axes = plt.subplots(\n",
    "    len(categorical_features),\n",
    "    2,\n",
    "    figsize=(15, 5 * len(categorical_features)),\n",
    "    squeeze=False,\n",
    ")\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    # Before Matching - Feb 2023 Data\n",
    "    sns.countplot(x=feature, hue=\"group_assignment\", data=feb_2023_data, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f\"Before Matching (Feb 2023) - {feature}\")\n",
    "    axes[i, 0].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[i, 0].set_xlabel(\"\")  # Remove x-label to save space\n",
    "\n",
    "    # After Matching\n",
    "    sns.countplot(\n",
    "        x=feature, hue=\"group_assignment\", data=final_matched_dataset, ax=axes[i, 1]\n",
    "    )\n",
    "    axes[i, 1].set_title(f\"After Matching - {feature}\")\n",
    "    axes[i, 1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[i, 1].set_xlabel(\"\")  # Remove x-label to save space\n",
    "\n",
    "plt.tight_layout(h_pad=3)  # Adjust vertical spacing to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# Now, plotting numerical features with KDE plots\n",
    "fig, axes = plt.subplots(\n",
    "    len(numerical_features), 2, figsize=(15, 5 * len(numerical_features)), squeeze=False\n",
    ")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    # Before Matching - Feb 2023 Data\n",
    "    sns.kdeplot(\n",
    "        data=feb_2023_data,\n",
    "        x=feature,\n",
    "        hue=\"group_assignment\",\n",
    "        ax=axes[i, 0],\n",
    "        common_norm=False,\n",
    "    )\n",
    "    axes[i, 0].set_title(f\"Before Matching (Feb 2023) - {feature}\")\n",
    "\n",
    "    # After Matching\n",
    "    sns.kdeplot(\n",
    "        data=final_matched_dataset,\n",
    "        x=feature,\n",
    "        hue=\"group_assignment\",\n",
    "        ax=axes[i, 1],\n",
    "        common_norm=False,\n",
    "    )\n",
    "    axes[i, 1].set_title(f\"After Matching - {feature}\")\n",
    "\n",
    "plt.tight_layout(h_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Filter for '2023-02-01' in 'data' for before matching, and use 'final_matched_dataset' for after matching\n",
    "before_matching_data = data[data[\"ds\"] == \"2023-02-01\"][\n",
    "    [\"group_assignment\", \"cluster_mrr_total\"]\n",
    "].copy()\n",
    "before_matching_data[\"Match_Status\"] = \"Before Matching\"\n",
    "\n",
    "after_matching_data = final_matched_dataset[\n",
    "    [\"group_assignment\", \"cluster_mrr_total\"]\n",
    "].copy()\n",
    "after_matching_data[\"Match_Status\"] = \"After Matching\"\n",
    "\n",
    "# Combine and plot without outliers\n",
    "combined_data = pd.concat([before_matching_data, after_matching_data])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x=\"Match_Status\",\n",
    "    y=\"cluster_mrr_total\",\n",
    "    hue=\"group_assignment\",\n",
    "    data=combined_data,\n",
    "    showfliers=False,\n",
    ")\n",
    "plt.title(\"Cluster MRR Total Before and After Matching\")\n",
    "plt.xlabel(\"Matching Status\")\n",
    "plt.ylabel(\"Cluster MRR Total\")\n",
    "plt.legend(title=\"Group Assignment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Copy df_raw to avoid altering the original DataFrame\n",
    "data = df_raw.copy()\n",
    "\n",
    "# Ensure 'ds' columns in the copied data and final_matched_dataset are datetime objects\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "final_matched_dataset[\"ds\"] = pd.to_datetime(final_matched_dataset[\"ds\"])\n",
    "\n",
    "# Merge final_matched_dataset with the filtered copied data on 'cluster_id'\n",
    "merged_data = pd.merge(\n",
    "    final_matched_dataset[[\"cluster_id\", \"ds\"]],\n",
    "    data[[\"cluster_id\", \"ds\", \"cluster_consumption_revenue\"]],\n",
    "    on=\"cluster_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_final\", \"_raw\"),\n",
    ")\n",
    "\n",
    "# Filter to include records where 'ds_raw' is equal to or after 'ds_final'\n",
    "filtered_data = merged_data[merged_data[\"ds_raw\"] >= merged_data[\"ds_final\"]].copy()\n",
    "\n",
    "# Calculate the month difference\n",
    "filtered_data[\"month_diff\"] = (\n",
    "    (filtered_data[\"ds_raw\"].dt.year - filtered_data[\"ds_final\"].dt.year) * 12\n",
    "    + filtered_data[\"ds_raw\"].dt.month\n",
    "    - filtered_data[\"ds_final\"].dt.month\n",
    ")\n",
    "\n",
    "# Pivot the table based on 'month_diff' for MRR totals\n",
    "pivot_mrr_totals = filtered_data.pivot_table(\n",
    "    index=\"cluster_id\",\n",
    "    columns=\"month_diff\",\n",
    "    values=\"cluster_consumption_revenue\",\n",
    "    aggfunc=\"first\",\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the columns and rename them to \"Month X\"\n",
    "pivot_mrr_totals.columns = [\"cluster_id\"] + [\n",
    "    f\"Month {col}\" if isinstance(col, int) else col\n",
    "    for col in pivot_mrr_totals.columns[1:]\n",
    "]\n",
    "\n",
    "# Merge the pivot table back into final_matched_dataset\n",
    "final_dataset_with_monthly_mrr = pd.merge(\n",
    "    final_matched_dataset, pivot_mrr_totals, on=\"cluster_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Fixed date for imputation\n",
    "fixed_date = datetime(2024, 2, 1)\n",
    "mrr_columns = [col for col in final_dataset_with_monthly_mrr.columns if \"Month\" in col]\n",
    "\n",
    "\n",
    "# Function to impute MRR values\n",
    "def impute_mrr(row):\n",
    "    for col in mrr_columns:\n",
    "        month_num = int(col.split(\" \")[1]) if \"Month\" in col else 0\n",
    "        month_diff = (\n",
    "            (fixed_date.year - row[\"ds\"].year) * 12 + fixed_date.month - row[\"ds\"].month\n",
    "        )\n",
    "        if pd.isnull(row[col]) and month_num <= month_diff:\n",
    "            row[col] = 0\n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the imputation row-wise\n",
    "final_dataset_with_monthly_mrr = final_dataset_with_monthly_mrr.apply(\n",
    "    impute_mrr, axis=1\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "final_dataset_with_monthly_mrr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import sem, t, mstats\n",
    "\n",
    "# Dynamically generate MRR columns\n",
    "mrr_columns = [col for col in final_dataset_with_monthly_mrr.columns if \"Month\" in col]\n",
    "mrr_columns = sorted(mrr_columns, key=lambda x: int(x.split(\" \")[-1]))[:12]\n",
    "\n",
    "\n",
    "def calculate_means_and_ci(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data) if n > 1 else 0\n",
    "    margin = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "\n",
    "def winsorize_by_month(data, mrr_columns):\n",
    "    for month in mrr_columns:\n",
    "        # Select non-zero and non-null values for winsorization\n",
    "        valid_values = data[(data[month] != 0) & (data[month].notna())][month]\n",
    "        winsorized_values = mstats.winsorize(valid_values, limits=[0.05, 0.05])\n",
    "\n",
    "        # Update the dataframe with winsorized values for non-zero and non-null entries\n",
    "        data.loc[(data[month] != 0) & (data[month].notna()), month] = winsorized_values\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply winsorization to the dataset\n",
    "final_dataset_with_monthly_mrr = winsorize_by_month(\n",
    "    final_dataset_with_monthly_mrr, mrr_columns\n",
    ")\n",
    "\n",
    "original_group_colors = {\"treatment group\": \"red\", \"control group\": \"skyblue\"}\n",
    "group_rename = {\n",
    "    \"treatment group\": \"Clusters w/ Product\",\n",
    "    \"control group\": \"Clusters w/o Product\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for original_group, color in original_group_colors.items():\n",
    "    new_group_name = group_rename[original_group]\n",
    "    means = []\n",
    "    lower_cis = []\n",
    "    upper_cis = []\n",
    "    for month in mrr_columns:  # Loop through the filtered/selected month columns\n",
    "        month_data = final_dataset_with_monthly_mrr[\n",
    "            (final_dataset_with_monthly_mrr[\"group_assignment\"] == original_group)\n",
    "            & final_dataset_with_monthly_mrr[month].notna()\n",
    "        ][month]\n",
    "        mean, lower_ci, upper_ci = calculate_means_and_ci(month_data)\n",
    "        means.append(mean)\n",
    "        lower_cis.append(lower_ci)\n",
    "        upper_cis.append(upper_ci)\n",
    "\n",
    "    plt.plot(mrr_columns, means, marker=\"o\", label=new_group_name, color=color)\n",
    "    plt.fill_between(mrr_columns, lower_cis, upper_cis, color=color, alpha=0.2)\n",
    "\n",
    "    for i, mean in enumerate(means):\n",
    "        plt.text(\n",
    "            mrr_columns[i],\n",
    "            mean + (max(upper_cis) - min(lower_cis)) * 0.05,\n",
    "            f\"${mean:.0f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "plt.title(\"Monthly Average MRR Since Clusters First Acquired the Product\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average MRR ($)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "group_means = {}\n",
    "\n",
    "for original_group in [\"treatment group\", \"control group\"]:\n",
    "    means = []\n",
    "    lower_cis = []\n",
    "    upper_cis = []\n",
    "    for month in mrr_columns:\n",
    "        month_data = final_dataset_with_monthly_mrr[\n",
    "            (final_dataset_with_monthly_mrr[\"group_assignment\"] == original_group)\n",
    "            & final_dataset_with_monthly_mrr[month].notna()\n",
    "        ][month]\n",
    "        mean, lower_ci, upper_ci = calculate_means_and_ci(month_data)\n",
    "        means.append(mean)\n",
    "        lower_cis.append(lower_ci)\n",
    "        upper_cis.append(upper_ci)\n",
    "    group_means[original_group] = (means, lower_cis, upper_cis)\n",
    "\n",
    "# Calculate percentage of control group mean MRR to treatment group mean MRR\n",
    "treatment_means, _, _ = group_means[\"treatment group\"]\n",
    "control_means, _, _ = group_means[\"control group\"]\n",
    "percentage_means = [\n",
    "    (control / treatment) * 100 if treatment else 0\n",
    "    for control, treatment in zip(control_means, treatment_means)\n",
    "]\n",
    "\n",
    "# Calculate the average of the percentage means\n",
    "average_percentage_mean = np.mean(percentage_means[1:])\n",
    "\n",
    "# Plot the percentage line in green\n",
    "plt.plot(\n",
    "    mrr_columns,\n",
    "    percentage_means,\n",
    "    marker=\"o\",\n",
    "    color=\"green\",\n",
    "    label=\"Revenue Impact Baseline\",\n",
    ")\n",
    "\n",
    "# Adding data labels for the percentage\n",
    "for i, txt in enumerate(percentage_means):\n",
    "    plt.annotate(\n",
    "        f\"{txt:.0f}%\",\n",
    "        (mrr_columns[i], percentage_means[i]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "# Plot the average of the percentage means as a horizontal line\n",
    "plt.axhline(\n",
    "    y=average_percentage_mean, color=\"r\", linestyle=\"--\", label=f\"Baseline Average\"\n",
    ")\n",
    "\n",
    "# Add a label for the average line\n",
    "# Adjust the x position as needed to place the label at the desired location on your chart\n",
    "x_position_for_average_label = len(mrr_columns) - 10  # This puts it at the last month\n",
    "plt.text(\n",
    "    x_position_for_average_label,\n",
    "    average_percentage_mean,\n",
    "    f\" Average (excl Month 0): {average_percentage_mean:.0f}%\",\n",
    "    verticalalignment=\"top\",\n",
    "    horizontalalignment=\"right\",\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.title(\"Monthly MRR of Clusters w/o Product as a Percentage of Clusters w/ Product\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Increase the y-axis limit\n",
    "plt.ylim(bottom=0, top=120)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing from the previous script\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for original_group, color in original_group_colors.items():\n",
    "    sample_sizes = []  # List to hold the sample size for each month\n",
    "    for month in mrr_columns:\n",
    "        # For each month, calculate the sample size of the non-null data for the current group\n",
    "        sample_size = len(\n",
    "            final_dataset_with_monthly_mrr[\n",
    "                (final_dataset_with_monthly_mrr[\"group_assignment\"] == original_group)\n",
    "                & final_dataset_with_monthly_mrr[month].notna()\n",
    "            ][month]\n",
    "        )\n",
    "        sample_sizes.append(sample_size)\n",
    "\n",
    "    plt.plot(\n",
    "        mrr_columns,\n",
    "        sample_sizes,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=group_rename[original_group],\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    for i, size in enumerate(sample_sizes):\n",
    "        plt.text(\n",
    "            mrr_columns[i],\n",
    "            size,\n",
    "            f\"{size}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "plt.title(\"Sample Size per Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sample Size\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #1 - Static Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use previous percentage means list\n",
    "# percentage_means = [\n",
    "#     97.804844047644,\n",
    "#     92.62622004657587,\n",
    "#     87.52743197833564,\n",
    "#     84.75675652189551,\n",
    "#     83.95310838499024,\n",
    "#     81.1257192970022,\n",
    "#     79.40193019770567,\n",
    "#     77.0675077182838,\n",
    "#     76.57380869819313,\n",
    "#     76.95478784212231,\n",
    "#     72.10422499840014,\n",
    "#     74.39279915640566,\n",
    "# ]\n",
    "\n",
    "# # Calculate the average of the percentage means\n",
    "# average_percentage_mean = np.mean(percentage_means[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "data = df_raw.copy()\n",
    "\n",
    "# Ensure both 'ds' and product_ds columns in 'data' DataFrame are in datetime format\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "data[product_ds] = pd.to_datetime(data[product_ds])\n",
    "\n",
    "# Filter rows based on the condition that product_ds >= 'ds' and product_ds < '2024-02-01'\n",
    "filtered_data = data[\n",
    "    (data[product_ds].dt.to_period(\"M\") <= data[\"ds\"].dt.to_period(\"M\"))\n",
    "    & (data[\"ds\"] <= pd.Timestamp(\"2024-01-01\"))\n",
    "]\n",
    "\n",
    "# Calculate the total sum of MRR for the filtered data\n",
    "total_mrr_sum = filtered_data[\"cluster_consumption_revenue\"].sum()\n",
    "\n",
    "# Print the total sum of MRR formatted with no decimals\n",
    "print(f\"Total sum of MRR for filtered clusters: ${total_mrr_sum:,.0f}\")\n",
    "\n",
    "# Calculate and print the number of unique clusters\n",
    "unique_clusters_count = filtered_data[\"cluster_id\"].nunique()\n",
    "print(f\"Number of unique clusters: {unique_clusters_count}\")\n",
    "\n",
    "# Get the baseline MRR percentage from 'average_percentage_mean'\n",
    "baseline_mrr_percentage = average_percentage_mean\n",
    "\n",
    "# Print the baseline MRR percentage with no decimals\n",
    "print(f\"Baseline MRR Percentage (average): {baseline_mrr_percentage:.0f}%\")\n",
    "\n",
    "# Calculate the incremental revenue percentage\n",
    "incremental_revenue_percentage = 100 - baseline_mrr_percentage\n",
    "\n",
    "# Print the incremental revenue percentage with no decimals\n",
    "print(\n",
    "    f\"Incremental Revenue Percentage (1 - Baseline): {incremental_revenue_percentage:.0f}%\"\n",
    ")\n",
    "\n",
    "# Calculate incremental revenue from the usage of product\n",
    "incremental_revenue = total_mrr_sum * ((1 - baseline_mrr_percentage / 100))\n",
    "\n",
    "# Print the incremental revenue with no decimals\n",
    "print(f\"Incremental revenue from product usage: ${incremental_revenue:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #2: Dynamic Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Copy original DataFrame\n",
    "data = df_raw.copy()\n",
    "\n",
    "# Convert to datetime format and adjust to the first day of the month\n",
    "data[\"ds\"] = pd.to_datetime(data[\"ds\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "data[product_ds] = pd.to_datetime(data[product_ds]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# Filter data based on conditions\n",
    "filtered_data = data[\n",
    "    (data[product_ind] == 1)\n",
    "    & (data[product_ds] <= data[\"ds\"])  # Direct comparison using first day of the month\n",
    "    & (data[\"ds\"] >= \"2023-02-01\")  # Start date filter\n",
    "    & (data[\"ds\"] <= \"2024-01-31\")  # End date filter\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Calculate total month difference between 'ds' and 'product_ds' columns considering years, and cap at 11\n",
    "filtered_data[\"month_diff\"] = filtered_data.apply(\n",
    "    lambda row: min(\n",
    "        relativedelta(row[\"ds\"], row[product_ds]).years * 12\n",
    "        + relativedelta(row[\"ds\"], row[product_ds]).months,\n",
    "        11,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Group by month difference and sum total cluster_consumption_revenue\n",
    "monthly_mrr = (\n",
    "    filtered_data.groupby(\"month_diff\")[\"cluster_consumption_revenue\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Create a copy of the DataFrame to work with\n",
    "adjusted_mrr = monthly_mrr.copy()\n",
    "\n",
    "# Add original total revenue to the copy for reference and rename the columns\n",
    "adjusted_mrr[\"original_revenue\"] = adjusted_mrr[\"cluster_consumption_revenue\"]\n",
    "adjusted_mrr.rename(\n",
    "    columns={\n",
    "        \"month_diff\": \"product_tenure\",\n",
    "        \"cluster_consumption_revenue\": \"attributed_consumption_revenue\",\n",
    "        \"original_revenue\": \"cluster_revenue\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Update the code to include the percentage columns and adjusted revenue\n",
    "for i, baseline_percentage in enumerate(percentage_means):\n",
    "    month = i\n",
    "    if month >= 0:\n",
    "        # Calculate the adjusted revenue\n",
    "        adjusted_factor = 1 - (baseline_percentage / 100)\n",
    "        adjusted_revenue = (\n",
    "            adjusted_mrr.loc[\n",
    "                adjusted_mrr[\"product_tenure\"] == month,\n",
    "                \"attributed_consumption_revenue\",\n",
    "            ]\n",
    "            * adjusted_factor\n",
    "        )\n",
    "\n",
    "        # Update the DataFrame with the new adjusted revenue\n",
    "        adjusted_mrr.loc[\n",
    "            adjusted_mrr[\"product_tenure\"] == month, \"attributed_consumption_revenue\"\n",
    "        ] = adjusted_revenue\n",
    "\n",
    "        # Store the percentage and 1-percentage with two decimal places\n",
    "        adjusted_mrr.loc[\n",
    "            adjusted_mrr[\"product_tenure\"] == month, \"baseline_percentage\"\n",
    "        ] = f\"{baseline_percentage:.2f}%\"\n",
    "        adjusted_mrr.loc[\n",
    "            adjusted_mrr[\"product_tenure\"] == month, \"attributed_revenue_percentage\"\n",
    "        ] = f\"{adjusted_factor:.2%}\"\n",
    "\n",
    "# Calculate the sum of the adjusted and original revenues\n",
    "total_adjusted_revenue = adjusted_mrr[\"attributed_consumption_revenue\"].sum()\n",
    "total_cluster_revenue = adjusted_mrr[\"cluster_revenue\"].sum()\n",
    "\n",
    "# Format the revenue columns as currency in the adjusted DataFrame\n",
    "adjusted_mrr[\"cluster_revenue\"] = adjusted_mrr[\"cluster_revenue\"].apply(\n",
    "    lambda x: f\"${x:,.2f}\"\n",
    ")\n",
    "adjusted_mrr[\"attributed_consumption_revenue\"] = adjusted_mrr[\n",
    "    \"attributed_consumption_revenue\"\n",
    "].apply(lambda x: f\"${x:,.2f}\")\n",
    "\n",
    "# Append total rows for both revenues\n",
    "total_row = pd.DataFrame(\n",
    "    {\n",
    "        \"product_tenure\": [\"Total\"],\n",
    "        \"cluster_revenue\": [f\"${total_cluster_revenue:,.2f}\"],\n",
    "        \"attributed_consumption_revenue\": [f\"${total_adjusted_revenue:,.2f}\"],\n",
    "    }\n",
    ")\n",
    "adjusted_mrr = pd.concat([adjusted_mrr, total_row], ignore_index=True)\n",
    "\n",
    "# Print the adjusted DataFrame using tabulate for a nicer table format\n",
    "print(tabulate(adjusted_mrr, headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "# Print the total revenues with formatting\n",
    "print(f\"\\nTotal Cluster Revenue: ${total_cluster_revenue:,.2f}\")\n",
    "print(f\"Total Attributed Consumption Revenue: ${total_adjusted_revenue:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Product Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from itertools import combinations\n",
    "\n",
    "# # Create a copy of the original DataFrame\n",
    "# data = df_raw.copy()\n",
    "\n",
    "# # Define product columns based on your data sample\n",
    "# product_columns = [\n",
    "#     \"text_search_ind\",\n",
    "#     \"vector_search_ind\",\n",
    "#     \"timeseries_ind\",\n",
    "#     \"sync_ind\",\n",
    "#     \"triggers_functions_ind\",\n",
    "#     \"charts_ind\",\n",
    "#     \"onlinearchive_ind\",\n",
    "#     \"adf_ind\",\n",
    "#     \"sql_ind\",\n",
    "# ]\n",
    "\n",
    "# # Total unique clusters\n",
    "# total_unique_clusters = data[\"cluster_id\"].nunique()\n",
    "\n",
    "\n",
    "# # Helper function to find clusters with >=N products\n",
    "# def find_clusters_with_n_or_more_products(df, product_columns, n):\n",
    "#     clusters = set()\n",
    "#     for combo_length in range(n, len(product_columns) + 1):\n",
    "#         for combo in combinations(product_columns, combo_length):\n",
    "#             temp_df = df[df[list(combo)].sum(axis=1) >= n]\n",
    "#             clusters.update(temp_df[\"cluster_id\"].unique())\n",
    "#     return clusters\n",
    "\n",
    "\n",
    "# # Clusters with at least 1 product\n",
    "# clusters_with_at_least_1_product = find_clusters_with_n_or_more_products(\n",
    "#     data, product_columns, 1\n",
    "# )\n",
    "\n",
    "# # Clusters with at least 2 products\n",
    "# clusters_with_at_least_2_products = find_clusters_with_n_or_more_products(\n",
    "#     data, product_columns, 2\n",
    "# )\n",
    "\n",
    "# # Clusters with at least 3 products\n",
    "# clusters_with_at_least_3_products = find_clusters_with_n_or_more_products(\n",
    "#     data, product_columns, 3\n",
    "# )\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Total Unique Clusters: {total_unique_clusters}\")\n",
    "# print(\n",
    "#     f\"Unique Clusters with at Least 1 Product: {len(clusters_with_at_least_1_product)}\"\n",
    "# )\n",
    "# print(\n",
    "#     f\"Unique Clusters with at Least 2 Products: {len(clusters_with_at_least_2_products)}\"\n",
    "# )\n",
    "# print(\n",
    "#     f\"Unique Clusters with at Least 3 Products: {len(clusters_with_at_least_3_products)}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Total MRR for all clusters\n",
    "# data = data[data[\"ds\"] <= \"2024-01-01\"].copy()\n",
    "# total_mrr_all_clusters = data[\"cluster_consumption_revenue\"].sum()\n",
    "\n",
    "# # Calculating MRR and counting unique clusters\n",
    "# mrr_1_product = data[data[\"cluster_id\"].isin(clusters_with_at_least_1_product)][\n",
    "#     \"cluster_consumption_revenue\"\n",
    "# ].sum()\n",
    "# count_1_product = len(clusters_with_at_least_1_product)\n",
    "\n",
    "# mrr_2_products = data[data[\"cluster_id\"].isin(clusters_with_at_least_2_products)][\n",
    "#     \"cluster_consumption_revenue\"\n",
    "# ].sum()\n",
    "# count_2_products = len(clusters_with_at_least_2_products)\n",
    "\n",
    "# mrr_3_products = data[data[\"cluster_id\"].isin(clusters_with_at_least_3_products)][\n",
    "#     \"cluster_consumption_revenue\"\n",
    "# ].sum()\n",
    "# count_3_products = len(clusters_with_at_least_3_products)\n",
    "\n",
    "# # Calculating percentages\n",
    "# percentage_mrr_1_product = (mrr_1_product / total_mrr_all_clusters) * 100\n",
    "# percentage_mrr_2_products = (mrr_2_products / total_mrr_all_clusters) * 100\n",
    "# percentage_mrr_3_products = (mrr_3_products / total_mrr_all_clusters) * 100\n",
    "\n",
    "# # Print the results with unique cluster counts\n",
    "# print(f\"Total MRR for all clusters: ${total_mrr_all_clusters:,.0f}\")\n",
    "# print(\n",
    "#     f\"Clusters with at least 1 product: {count_1_product}, Total MRR: ${mrr_1_product:,.0f} ({percentage_mrr_1_product:.2f}%)\"\n",
    "# )\n",
    "# print(\n",
    "#     f\"Clusters with at least 2 products: {count_2_products}, Total MRR: ${mrr_2_products:,.0f} ({percentage_mrr_2_products:.2f}%)\"\n",
    "# )\n",
    "# print(\n",
    "#     f\"Clusters with at least 3 products: {count_3_products}, Total MRR: ${mrr_3_products:,.0f} ({percentage_mrr_3_products:.2f}%)\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'final_dataset_with_monthly_mrr' is your DataFrame and is already loaded\n",
    "\n",
    "# Iterate over the DataFrame to create churn indicator columns based on the specified conditions\n",
    "for month in range(13):\n",
    "    mrr_col = f'Month {month}'  # Corrected column names\n",
    "    churn_col = f'Month {month}_churn'\n",
    "    \n",
    "    # Define conditions for churn indicator\n",
    "    conditions = [\n",
    "        (final_dataset_with_monthly_mrr[mrr_col] == 0),  # Churn if value is 0\n",
    "        (final_dataset_with_monthly_mrr[mrr_col].notnull())  # Not churn if value is not 0 and not null\n",
    "    ]\n",
    "    choices = [1, 0]  # Churn indicator values\n",
    "    final_dataset_with_monthly_mrr[churn_col] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "# Initialize an empty DataFrame to store churn rates\n",
    "churn_rates = pd.DataFrame()\n",
    "\n",
    "# List of churn indicator columns\n",
    "churn_indicator_columns = [f'Month {i}_churn' for i in range(13)]\n",
    "\n",
    "# Calculate churn rates\n",
    "for month_idx, col in enumerate(churn_indicator_columns):\n",
    "    # Group by 'group_assignment' and calculate churned clusters and total clusters with data\n",
    "    churn_data = final_dataset_with_monthly_mrr.groupby('group_assignment')[col].agg(\n",
    "        churned_clusters=np.sum,\n",
    "        total_clusters_with_data=pd.Series.count\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate churn rate\n",
    "    churn_data['churn_rate'] = (churn_data['churned_clusters'] / churn_data['total_clusters_with_data']) * 100\n",
    "    churn_data['month'] = f'Month {month_idx}'\n",
    "\n",
    "    # Append to the churn_rates DataFrame\n",
    "    churn_rates = pd.concat([churn_rates, churn_data], ignore_index=True)\n",
    "\n",
    "# Filter to include only Month 0 to Month 11\n",
    "churn_rates = churn_rates[churn_rates['month'].isin([f'Month {i}' for i in range(12)])]\n",
    "\n",
    "# Set seaborn style for the plot\n",
    "sns.set(style=\"white\", rc={\"axes.grid\": False})\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "lineplot = sns.lineplot(data=churn_rates, x='month', y='churn_rate', hue='group_assignment', marker='o')\n",
    "\n",
    "# Plot enhancements\n",
    "plt.title('Monthly Churn Rate for Control & Treatment Groups', fontsize=15)\n",
    "plt.xlabel('Month', fontsize=15)\n",
    "plt.ylabel('Churn Rate (%)', fontsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Group Assignment', title_fontsize='10', fontsize='11')\n",
    "\n",
    "# Adding data labels\n",
    "for index, row in churn_rates.iterrows():\n",
    "    plt.text(x=row['month'], y=row['churn_rate']+0.5,  # Slightly adjust the height for visibility\n",
    "             s=f'{row[\"churn_rate\"]:.0f}%', ha='center', color='black', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data = df_raw.copy()\n",
    "# data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "\n",
    "# # Reference date\n",
    "# reference_date = pd.to_datetime(\"2023-02-01\")\n",
    "\n",
    "# # Extract year and month for both `ds` and `reference_date`, then calculate the month difference\n",
    "# data[\"year\"], data[\"month\"] = data[\"ds\"].dt.year, data[\"ds\"].dt.month\n",
    "# ref_year, ref_month = reference_date.year, reference_date.month\n",
    "\n",
    "# # Calculate month difference as 12 times the year difference plus the month difference\n",
    "# data[\"month_diff\"] = (data[\"year\"] - ref_year) * 12 + (data[\"month\"] - ref_month)\n",
    "\n",
    "# # Now, you can pivot your table with this adjusted 'month_diff'\n",
    "# pivot_table = data.pivot_table(\n",
    "#     index=\"cluster_id\",\n",
    "#     columns=\"month_diff\",\n",
    "#     values=\"cluster_consumption_revenue\",\n",
    "#     fill_value=0,\n",
    "#     aggfunc=\"sum\",\n",
    "# ).reset_index()\n",
    "\n",
    "# # Clean-up: You might want to drop the temporary columns 'year' and 'month' if they are no longer needed\n",
    "# data.drop(columns=[\"year\", \"month\"], inplace=True)\n",
    "\n",
    "# # Filter rows where month 0 is not zero\n",
    "# pivot_table = pivot_table[pivot_table[0] != 0]\n",
    "\n",
    "# # Calculate churn rates for each month from 0 to 11\n",
    "# churn_rates = []\n",
    "# for month in range(12):  # Iterate through months 0 to 11\n",
    "#     churn_count = pivot_table[pivot_table[month] == 0].shape[0]\n",
    "#     total_count = pivot_table.shape[0]\n",
    "#     churn_rate = churn_count / total_count\n",
    "#     churn_rates.append(churn_rate)\n",
    "\n",
    "# # Plot churn rates\n",
    "# plt.figure(figsize=(14, 8))  # Set figure size\n",
    "# plt.plot(range(12), churn_rates, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "\n",
    "# # Adding data labels for each point\n",
    "# for i, rate in enumerate(churn_rates):\n",
    "#     plt.text(i, rate, f\"{rate:.0%}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# plt.title(\"Churn Rate for 2023-02 Active Cluster Cohort\", fontsize=15)\n",
    "# plt.xlabel(\"Month\")\n",
    "# plt.ylabel(\"Churn Rate\")\n",
    "# plt.xticks(range(12), labels=[f\"Month {i}\" for i in range(12)])\n",
    "# plt.grid(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data = df_raw.copy()\n",
    "\n",
    "# data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n",
    "# data[\"created_at\"] = pd.to_datetime(\n",
    "#     data[\"created_at\"]\n",
    "# )  # Ensure 'created_at' is in datetime format\n",
    "\n",
    "# # Narrow down to the population created in February 2023\n",
    "# start_date = pd.to_datetime(\"2023-02-01\")\n",
    "# end_date = pd.to_datetime(\"2023-02-28\")\n",
    "# data = data[(data[\"created_at\"] >= start_date) & (data[\"created_at\"] <= end_date)]\n",
    "\n",
    "# # Reference date\n",
    "# reference_date = pd.to_datetime(\"2023-02-01\")\n",
    "\n",
    "# # Extract year and month for both `ds` and `reference_date`, then calculate the month difference\n",
    "# data[\"year\"], data[\"month\"] = data[\"ds\"].dt.year, data[\"ds\"].dt.month\n",
    "# ref_year, ref_month = reference_date.year, reference_date.month\n",
    "\n",
    "# # Calculate month difference as 12 times the year difference plus the month difference\n",
    "# data[\"month_diff\"] = (data[\"year\"] - ref_year) * 12 + (data[\"month\"] - ref_month)\n",
    "\n",
    "# # Now, you can pivot your table with this adjusted 'month_diff'\n",
    "# pivot_table = data.pivot_table(\n",
    "#     index=\"cluster_id\",\n",
    "#     columns=\"month_diff\",\n",
    "#     values=\"cluster_consumption_revenue\",\n",
    "#     fill_value=0,\n",
    "#     aggfunc=\"sum\",\n",
    "# ).reset_index()\n",
    "\n",
    "# # Clean-up: You might want to drop the temporary columns 'year' and 'month' if they are no longer needed\n",
    "# data.drop(columns=[\"year\", \"month\"], inplace=True)\n",
    "\n",
    "# # Filter rows where month 0 is not zero\n",
    "# pivot_table = pivot_table[pivot_table[0] != 0]\n",
    "\n",
    "# # Calculate churn rates for each month from 0 to 11\n",
    "# churn_rates = []\n",
    "# for month in range(12):  # Iterate through months 0 to 11\n",
    "#     churn_count = pivot_table[pivot_table[month] == 0].shape[0]\n",
    "#     total_count = pivot_table.shape[0]\n",
    "#     churn_rate = churn_count / total_count\n",
    "#     churn_rates.append(churn_rate)\n",
    "\n",
    "# # Plot churn rates\n",
    "# plt.figure(figsize=(14, 8))  # Set figure size\n",
    "# plt.plot(range(12), churn_rates, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "\n",
    "# # Adding data labels for each point\n",
    "# for i, rate in enumerate(churn_rates):\n",
    "#     plt.text(i, rate, f\"{rate:.0%}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# plt.title(\"Churn Rate for Feb 2023 Created Cluster Cohort\", fontsize=15)\n",
    "# plt.xlabel(\"Month\")\n",
    "# plt.ylabel(\"Churn Rate\")\n",
    "# plt.xticks(range(12), labels=[f\"Month {i}\" for i in range(12)])\n",
    "# plt.grid(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data setup\n",
    "# products = [\"text_search\", \"vector_search\", \"time_series\", \"sync\", \"trigger and functions\",\n",
    "#             \"charts\", \"online archive\", \"adf\", \"sql\"]\n",
    "# unique_clusters = [5443, 365, 1424, 1109, 6097, 7931, 1976, 2194, 186]\n",
    "# definitions = [\"qualification\", \"qualification\", \"qualification\", \"qualification\",\n",
    "#                \"activation\", \"activation\", \"activation\", \"activation\", \"activation\"]\n",
    "\n",
    "# # Choose colors based on 'definitions'\n",
    "# colors = ['#1f77b4' if defi == \"qualification\" else '#ff7f0e' for defi in definitions]\n",
    "\n",
    "# # Create bar chart\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# bars = ax.barh(products, unique_clusters, color=colors)\n",
    "\n",
    "# # Adding grid, background color, and customizing axes\n",
    "# ax.set_facecolor('#f9f9f9')\n",
    "# ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# ax.set_axisbelow(True)\n",
    "\n",
    "# # Annotating small sample sizes\n",
    "# for bar, value in zip(bars, unique_clusters):\n",
    "#     if value < 1000:\n",
    "#         ax.annotate(f'Small sample: {value}', (value + 100, bar.get_y() + bar.get_height()/2),\n",
    "#                     textcoords=\"offset points\", xytext=(5,0), va='center', fontsize=8, color='red')\n",
    "\n",
    "# # Enhance labels and title\n",
    "# ax.set_xlabel('Number of Unique Clusters')\n",
    "# ax.set_title('Product Unique Clusters Overview', fontsize=14, fontweight='bold')\n",
    "# ax.xaxis.set_tick_params(labelsize=10)\n",
    "# ax.yaxis.set_tick_params(labelsize=10)\n",
    "\n",
    "# # Add legend with a title\n",
    "# qualification_bar = plt.Rectangle((0,0),1,1, fc=\"#1f77b4\", edgecolor='none')\n",
    "# activation_bar = plt.Rectangle((0,0),1,1, fc='#ff7f0e', edgecolor='none')\n",
    "# ax.legend([qualification_bar, activation_bar], ['Qualification', 'Activation'], loc='upper right', title=\"Definition Used\")\n",
    "\n",
    "# # Add concise note about the data context and adjust position\n",
    "# note_text = (\"Note: The numbers shown are the total clusters available for revenue distribution per product during FY24\")\n",
    "# plt.gcf().subplots_adjust(bottom=0.15)  # Adjust the bottom to prevent overlap\n",
    "# plt.figtext(0.5, 0.01, note_text, wrap=True, horizontalalignment='center', fontsize=10, color='darkred')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
